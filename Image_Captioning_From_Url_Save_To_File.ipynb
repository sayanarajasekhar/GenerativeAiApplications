{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkIFAw5FNhT3Cgzh6gxJcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanarajasekhar/GenerativeAiApplications/blob/main/Image_Captioning_From_Url_Save_To_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing automated image captioning tool\n",
        "\n",
        "Implementing an automated image captioning program that works directly from a URL. The user provides the URL, and the code generates captions for the images found on the webpage. The output is a text file that includes all the image URLs along with their respective captions"
      ],
      "metadata": {
        "id": "YEg3O6QBkH8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24gWq8dwjW99",
        "outputId": "2d06a649-9d05-47d6-a743-6920210f9f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status:  200 HTML size:  620778\n",
            "Found 43 <img> tags\n",
            "[1] Caption saved\n",
            "[5] Caption saved\n",
            "[11] Caption saved\n",
            "[12] Caption saved\n",
            "[13] Caption saved\n",
            "[14] Caption saved\n",
            "[15] Caption saved\n",
            "[16] Caption saved\n",
            "[17] Caption saved\n",
            "[18] Caption saved\n",
            "[19] Caption saved\n",
            "[20] Caption saved\n",
            "[21] Caption saved\n",
            "[22] Caption saved\n",
            "[23] Caption saved\n",
            "[39] Caption saved\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the pretrained processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# URL of the page to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/IBM\"\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "response = requests.get(url, headers = headers)\n",
        "print(\"Status: \", response.status_code, \"HTML size: \", len(response.text))\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "img_elements = soup.find_all(\"img\")\n",
        "print(f\"Found {len(img_elements)} <img> tags\")\n",
        "\n",
        "with open(\"caption.txt\", \"w\", encoding = \"utf-8\") as caption_file:\n",
        "    for idx, img_element in enumerate(img_elements, start = 1):\n",
        "        img_url = img_element.get(\"src\") or img_element.get(\"data-src\")\n",
        "        if not img_url and img_element.has_attr(\"scrset\"):\n",
        "            img_url = img_element['scrset'].split[0]\n",
        "        if not img_url:\n",
        "            continue\n",
        "\n",
        "        # Skip SVG's directly\n",
        "        if img_url.endswith('.svg') or 'svg' in img_url:\n",
        "            continue\n",
        "\n",
        "        # Fix relative URLs\n",
        "        if img_url.startswith(\"//\"):\n",
        "            img_url = \"https:\" + img_url\n",
        "        elif img_url.startswith(\"/\"):\n",
        "            img_url = \"https://en.wikipedia.org\" + img_url\n",
        "        elif not img_url.startswith(\"http\"):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            r = requests.get(img_url, timeout=10, headers=headers)\n",
        "            raw_image = Image.open(BytesIO(r.content))\n",
        "            # Skip very small images\n",
        "            if raw_image.size[0] * raw_image.size[1] < 200:\n",
        "                continue\n",
        "            raw_image = raw_image.convert(\"RGB\")\n",
        "            # Process the image with a text prompt\n",
        "            text = \"the image of\"\n",
        "            inputs = processor(images=raw_image, text=text, return_tensors=\"pt\")\n",
        "            out = model.generate(**inputs, max_new_tokens=50)\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "            caption_file.write(f\"{img_url}: {caption}\\n\")\n",
        "            print(f\"[{idx}] Caption saved\")\n",
        "        except OSError:\n",
        "            # Skip images PIL cannot open (SVG, ICO, corrupt files)\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"[{idx}] Error: {e}\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZ17GRJxj3hD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}